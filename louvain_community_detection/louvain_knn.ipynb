{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import astropy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "# import itertools\n",
    "# from palettable import wesanderson\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "import scipy\n",
    "# from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less_important_features = ['race_other', 'mood_or_anx', 'prev_study_oth_calc', 'race_asian', 'mood_anx', \n",
    "#                                  'multiple_birth', 'race_native_amer', 'gen_test_oth_calc', \n",
    "#                                  'gen_dx_oth_calc_self_report', 'growth_oth_calc', 'race_native_hawaiian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_cols = ['age_at_eval_months', 'age_at_eval_years', 'age_at_registration_months']\n",
    "race_cols = ['race_asian','race_african_amer', 'race_native_amer', 'race_native_hawaiian',\n",
    "       'race_white', 'race_other', 'hispanic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = pd.read_csv('./spark_cleaned_data_more_features.csv')\n",
    "# spark_df_asd = spark_df['asd']\n",
    "# id_cols = ['asd','subject_sp_id', 'respondent_sp_id', 'family_id', 'biomother_id','biofather_id']\n",
    "id_cols = ['subject_sp_id', 'respondent_sp_id', 'family_id', 'biomother_id','biofather_id']\n",
    "spark_df = spark_df.drop(id_cols, axis=1)\n",
    "# spark_df_selected_feat  = spark_df.drop(less_important_features, axis=1)\n",
    "# spark_df_selected_feat  = spark_df.drop(age_cols, axis=1)\n",
    "spark_df_selected_feat  = spark_df.drop(race_cols, axis=1)\n",
    "selected_features =  spark_df_selected_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df_selected_feat.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved feature importances\n",
    "# importances_df = pd.read_pickle('./more_features(mf)/sorted_feature_importances_RF_mf.pkl')\n",
    "importances_df = pd.read_pickle('./more_features(mf)/sorted_feature_importances_RF_mf_newest.pkl')\n",
    "\n",
    "features_sorted_by_importance = importances_df['feature'].values\n",
    "feat_importances = importances_df['importance'].values\n",
    "\n",
    "# Sort the features in the dataframe by importance and add the target variable\n",
    "spark_df_selected_feat = spark_df_selected_feat[np.append('asd', features_sorted_by_importance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sample_datasets(df, sample_size, n_samples=5):\n",
    "    \"\"\"\n",
    "    Function to sample the dataset n_samples times and return the indices of the samples\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the proportions of ASD and non-ASD patients\n",
    "    total_count = len(df)\n",
    "    asd_count = df['asd'].sum()\n",
    "    non_asd_count = total_count - asd_count\n",
    "    asd_proportion = asd_count / total_count\n",
    "\n",
    "    sample_dfs = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Determine the number of ASD and non-ASD patients needed for the sample\n",
    "        asd_sample_size = int(asd_proportion * sample_size)\n",
    "        non_asd_sample_size = sample_size - asd_sample_size\n",
    "\n",
    "        # Select a stratified sample\n",
    "        asd_sample_df = df[df['asd'] == 1].sample(n=asd_sample_size, random_state=i)\n",
    "        non_asd_sample_df = df[df['asd'] == 0].sample(n=non_asd_sample_size, random_state=i)\n",
    "\n",
    "        # Combine the samples to get the final stratified sample\n",
    "        final_sample_df = pd.concat([asd_sample_df, non_asd_sample_df])\n",
    "        sample_dfs.append(final_sample_df.reset_index(drop=False))\n",
    "\n",
    "    return sample_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asd', 'dev_lang', 'dev_lang_dis', 'sex', 'attn_behav', 'dev_speech',\n",
       "       'dev_soc_prag', 'birth_oth_calc', 'dev_motor', 'behav_adhd', 'dev_ld',\n",
       "       'neuro_oth_calc', 'psych_oth_calc', 'behav_odd', 'mood_dep',\n",
       "       'mood_soc_anx', 'mood_anx', 'mood_ocd', 'mood_sep_anx', 'mood_dmd',\n",
       "       'mood_bipol', 'multiple_birth', 'mood_or_anx',\n",
       "       'gen_dx_oth_calc_self_report', 'mood_hoard', 'behav_intermitt_explos',\n",
       "       'behav_conduct', 'dev_mutism'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_selected_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mood_hoard', 'dev_motor', 'mood_sep_anx', 'mood_soc_anx',\n",
      "       'birth_oth_calc', 'behav_intermitt_explos', 'behav_adhd', 'dev_lang',\n",
      "       'attn_behav', 'dev_soc_prag', 'dev_ld', 'mood_dmd', 'psych_oth_calc',\n",
      "       'dev_speech', 'mood_anx', 'mood_or_anx', 'asd', 'neuro_oth_calc', 'sex',\n",
      "       'dev_mutism', 'mood_ocd', 'mood_bipol', 'behav_odd', 'dev_lang_dis',\n",
      "       'gen_dx_oth_calc_self_report', 'mood_dep', 'behav_conduct',\n",
      "       'multiple_birth'],\n",
      "      dtype='object')\n",
      "(28,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(selected_features)\n",
    "print(selected_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asd</th>\n",
       "      <th>dev_lang</th>\n",
       "      <th>dev_lang_dis</th>\n",
       "      <th>sex</th>\n",
       "      <th>attn_behav</th>\n",
       "      <th>dev_speech</th>\n",
       "      <th>dev_soc_prag</th>\n",
       "      <th>birth_oth_calc</th>\n",
       "      <th>dev_motor</th>\n",
       "      <th>behav_adhd</th>\n",
       "      <th>...</th>\n",
       "      <th>mood_sep_anx</th>\n",
       "      <th>mood_dmd</th>\n",
       "      <th>mood_bipol</th>\n",
       "      <th>multiple_birth</th>\n",
       "      <th>mood_or_anx</th>\n",
       "      <th>gen_dx_oth_calc_self_report</th>\n",
       "      <th>mood_hoard</th>\n",
       "      <th>behav_intermitt_explos</th>\n",
       "      <th>behav_conduct</th>\n",
       "      <th>dev_mutism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131806</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131807</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131808</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131809</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131810</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131811 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        asd  dev_lang  dev_lang_dis  sex  attn_behav  dev_speech  \\\n",
       "0         1       1.0           1.0    1         1.0         0.0   \n",
       "1         0       0.0           0.0    0         1.0         0.0   \n",
       "2         0       0.0           0.0    0         0.0         0.0   \n",
       "3         1       0.0           0.0    1         1.0         0.0   \n",
       "4         1       0.0           0.0    1         0.0         0.0   \n",
       "...     ...       ...           ...  ...         ...         ...   \n",
       "131806    0       0.0           0.0    0         0.0         0.0   \n",
       "131807    1       1.0           1.0    1         0.0         0.0   \n",
       "131808    0       0.0           0.0    0         0.0         0.0   \n",
       "131809    1       0.0           0.0    0         1.0         0.0   \n",
       "131810    1       0.0           0.0    1         1.0         0.0   \n",
       "\n",
       "        dev_soc_prag  birth_oth_calc  dev_motor  behav_adhd  ...  \\\n",
       "0                0.0               0        0.0         1.0  ...   \n",
       "1                0.0               0        0.0         1.0  ...   \n",
       "2                0.0               0        0.0         0.0  ...   \n",
       "3                0.0               0        0.0         1.0  ...   \n",
       "4                0.0               0        0.0         0.0  ...   \n",
       "...              ...             ...        ...         ...  ...   \n",
       "131806           0.0               0        0.0         0.0  ...   \n",
       "131807           0.0               0        0.0         0.0  ...   \n",
       "131808           0.0               0        0.0         0.0  ...   \n",
       "131809           0.0               0        0.0         0.0  ...   \n",
       "131810           0.0               0        0.0         1.0  ...   \n",
       "\n",
       "        mood_sep_anx  mood_dmd  mood_bipol  multiple_birth  mood_or_anx  \\\n",
       "0                0.0       0.0         0.0               0          0.0   \n",
       "1                0.0       0.0         1.0               0          1.0   \n",
       "2                0.0       0.0         0.0               0          0.0   \n",
       "3                0.0       0.0         0.0               0          0.0   \n",
       "4                0.0       0.0         0.0               0          0.0   \n",
       "...              ...       ...         ...             ...          ...   \n",
       "131806           0.0       0.0         0.0               0          0.0   \n",
       "131807           0.0       0.0         0.0               0          0.0   \n",
       "131808           0.0       0.0         0.0               0          0.0   \n",
       "131809           0.0       0.0         1.0               0          1.0   \n",
       "131810           0.0       0.0         0.0               0          1.0   \n",
       "\n",
       "        gen_dx_oth_calc_self_report  mood_hoard  behav_intermitt_explos  \\\n",
       "0                                 0         0.0                     0.0   \n",
       "1                                 0         0.0                     0.0   \n",
       "2                                 0         0.0                     0.0   \n",
       "3                                 0         0.0                     0.0   \n",
       "4                                 0         0.0                     0.0   \n",
       "...                             ...         ...                     ...   \n",
       "131806                            0         0.0                     0.0   \n",
       "131807                            0         0.0                     0.0   \n",
       "131808                            0         0.0                     0.0   \n",
       "131809                            0         0.0                     0.0   \n",
       "131810                            0         0.0                     0.0   \n",
       "\n",
       "        behav_conduct  dev_mutism  \n",
       "0                 0.0         0.0  \n",
       "1                 0.0         0.0  \n",
       "2                 0.0         0.0  \n",
       "3                 0.0         0.0  \n",
       "4                 0.0         0.0  \n",
       "...               ...         ...  \n",
       "131806            0.0         0.0  \n",
       "131807            0.0         0.0  \n",
       "131808            0.0         0.0  \n",
       "131809            0.0         0.0  \n",
       "131810            0.0         0.0  \n",
       "\n",
       "[131811 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_selected_feat\n",
    "# spark_df_asd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Calculate cosine similarity\n",
    "# cosine_sim_matrix = cosine_similarity(spark_df_selected_feat)\n",
    "\n",
    "# # Convert the cosine similarity matrix to a DataFrame\n",
    "# cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=spark_df_selected_feat.index, columns=spark_df_selected_feat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df_selected_feat = spark_df_selected_feat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Assuming spark_df_selected_feat is your object\n",
    "# memory_usage = sys.getsizeof(spark_df_selected_feat)\n",
    "\n",
    "# print(f\"Memory usage: {memory_usage} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_hamming_similarity(df):\n",
    "    # Convert the DataFrame to a boolean array if not already\n",
    "    # data_bool = df.astype(bool)\n",
    "    data_bool = df\n",
    "    \n",
    "    # Compute pairwise Hamming distances\n",
    "    pairwise_dist = pdist(data_bool, metric='hamming')\n",
    "    \n",
    "    # Convert distances to similarities\n",
    "    pairwise_sim = 1 - pairwise_dist\n",
    "    \n",
    "    # Convert the condensed distance matrix to a square matrix\n",
    "    similarity_matrix = squareform(pairwise_sim)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=df.index, columns=df.index)\n",
    "    \n",
    "    return similarity_df\n",
    "\n",
    "# hamming_sim_df_50k = pairwise_hamming_similarity(spark_df_selected_feat.iloc[:50000])\n",
    "# 1.5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pairwise_weighted_hamming_similarity(df, weights):\n",
    "    '''\n",
    "    Compute pairwise weighted Hamming similarity between rows of a DataFrame.\n",
    "    \n",
    "    df: DataFrame containing binary features\n",
    "    weights: 1D array of normalised weights for each feature (weights must sum to 1)\n",
    "    '''\n",
    "\n",
    "    # Ensure weights array matches the number of columns in df\n",
    "    assert len(weights) == df.shape[1], \"Weights length must match number of features\"\n",
    "    \n",
    "    # Convert DataFrame to numpy array for efficiency\n",
    "    data_array = df.to_numpy()\n",
    "    \n",
    "    # Custom function to compute weighted Hamming similarity\n",
    "    def weighted_hamming(u, v):\n",
    "        # Calculate weighted distance as sum of weights where elements are the same\n",
    "        return np.sum(weights[u == v])\n",
    "    \n",
    "    # Compute pairwise similarities using the custom metric\n",
    "    pairwise_sim = pdist(data_array, metric=weighted_hamming)\n",
    "    \n",
    "    # Convert the condensed distance matrix to a square matrix\n",
    "    similarity_matrix = squareform(pairwise_sim)\n",
    "    # Force the diagonal values to 1 (because squareform leaves them as 0 as it's meant for distances)\n",
    "    np.fill_diagonal(similarity_matrix, 1)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=df.index, columns=df.index)\n",
    "    \n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamming_sim_df_80k = pairwise_hamming_similarity(spark_df_selected_feat.iloc[:80000])\n",
    "# 4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamming_sim_df_90k = pairwise_hamming_similarity(spark_df_selected_feat.iloc[:20000])\n",
    "# 5 mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Patient Similarity Network from similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_info(G):\n",
    "    \"\"\"\n",
    "    Prints basic information about the graph.\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph.\n",
    "    \"\"\"\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(\"Sample nodes:\", list(G.nodes)[:10])  # Print first 10 nodes as a sample\n",
    "    print(\"Sample edges:\", list(G.edges(data=True))[:10])  # Print first 10 edges as a sample\n",
    "    \n",
    "    # Check for self-loops\n",
    "    self_loops = list(nx.selfloop_edges(G))\n",
    "    if self_loops:\n",
    "        print(f\"Number of self-loops: {len(self_loops)}\")\n",
    "        print(\"Self-loops:\", self_loops)\n",
    "    else:\n",
    "        print(\"No self-loops in the graph.\")\n",
    "\n",
    "def calculate_average_clustering(G):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average clustering coefficient of the graph.\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph.\n",
    "    \"\"\"\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "    print(f\"Average clustering coefficient: {avg_clustering}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN graph builing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_neighbors(sim_matrix_df, k_neighbors):\n",
    "    '''\n",
    "    Helper function to get the k-nearest neighbors for each node in the graph\n",
    "    sim_matrix_df : pd.DataFrame containing the similarity matrix of patient data\n",
    "    k_neighbors : number of nearest neighbors to consider\n",
    "\n",
    "    retruns : dictionary where keys are the indices of the nodes/patients and values are lists of the indices of the k-nearest neighbors\n",
    "    e.g. {0: [3, 1, 4], 1: [0, 3, 2], 2: [3, 1, 0], 3: [0, 1, 2], 4: [0, 3, 1]}\n",
    "    '''\n",
    "    print('Using Eucledian distance')\n",
    "    # Compute euclidean distance matrix\n",
    "    dist_mtx = scipy.spatial.distance_matrix(sim_matrix_df.values ,  sim_matrix_df.values)\n",
    "    dist_mtx = pd.DataFrame(dist_mtx , index = sim_matrix_df.index , columns = sim_matrix_df.index)\n",
    "    # print(dist_mtx)\n",
    "    \n",
    "    k_neighbors_dict = {}\n",
    "    for node in dist_mtx.index:\n",
    "\n",
    "        neighbors = dist_mtx.loc[node].nsmallest(k_neighbors + 1).index.tolist() \n",
    "        try: \n",
    "            neighbors.remove(node) # Exclude the node itself\n",
    "        except ValueError:\n",
    "            pass\n",
    "            # print('ValueError: Node {} is not in top k neighbors'.format(node))\n",
    "            # print(dist_mtx.loc[node].nsmallest(k_neighbors + 1))\n",
    "        \n",
    "        k_neighbors_dict[node] = neighbors\n",
    "\n",
    "    # print(k_neighbors_dict)\n",
    "    return k_neighbors_dict\n",
    "\n",
    "def get_k_neighbors_directly(sim_matrix_df, k_neighbors):\n",
    "    '''\n",
    "    Helper function to get the k-nearest neighbors for each node in the graph\n",
    "    sim_matrix_df : pd.DataFrame containing the similarity matrix of patient data\n",
    "    k_neighbors : number of nearest neighbors to consider\n",
    "\n",
    "    retruns : dictionary where keys are the indices of the nodes/patients and values are lists of the indices of the k-nearest neighbors\n",
    "    e.g. {0: [3, 1, 4], 1: [0, 3, 2], 2: [3, 1, 0], 3: [0, 1, 2], 4: [0, 3, 1]}\n",
    "    '''\n",
    "    # Compute euclidean distance matrix\n",
    "    # dist_mtx = scipy.spatial.distance_matrix(sim_matrix_df.values ,  sim_matrix_df.values)\n",
    "    # dist_mtx = pd.DataFrame(dist_mtx , index = sim_matrix_df.index , columns = sim_matrix_df.index)\n",
    "    \n",
    "    # print('NO Eucledian distance')\n",
    "\n",
    "\n",
    "    k_neighbors_dict = {}\n",
    "    for node in sim_matrix_df.index:\n",
    "\n",
    "        # neighbors = sim_matrix_df.loc[node].nsmallest(k_neighbors + 1).index.tolist() \n",
    "        neighbors = sim_matrix_df.loc[node].nlargest(k_neighbors + 1).index.tolist() \n",
    "        try: \n",
    "            neighbors.remove(node) # Exclude the node itself\n",
    "        except ValueError:\n",
    "            pass\n",
    "            # print('ValueError: Node {} is not in top k neighbors'.format(node))\n",
    "            # print sim_matrix_df.loc[node].nsmallest(k_neighbors + 1))\n",
    "        \n",
    "        k_neighbors_dict[node] = neighbors\n",
    "        \n",
    "    # print(k_neighbors_dict)\n",
    "\n",
    "    return k_neighbors_dict\n",
    "\n",
    "def build_network_knn(sim_matrix_df , labels,  k_neighbors = 20, compute_knn_directly = True) : \n",
    "    '''\n",
    "    sim_matrix_df : pd.DataFrame containing the similarity matrix of patient data\n",
    "    labels : pd.Series indicating asd status for each patient\n",
    "    k_neighbors : number of nearest neighbors to consider\n",
    "    '''\n",
    "\n",
    "    if compute_knn_directly:\n",
    "        k_neighbors_dict = get_k_neighbors_directly(sim_matrix_df, k_neighbors)\n",
    "    else:\n",
    "        k_neighbors_dict = get_k_neighbors(sim_matrix_df, k_neighbors)\n",
    "\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    G.add_nodes_from(sim_matrix_df.index)\n",
    "\n",
    "    # nx.set_node_attributes(G , labels , 'label')\n",
    "    nx.set_node_attributes(G , labels , 'asd')\n",
    "    nx.set_node_attributes(G , pd.Series(np.arange(len(sim_matrix_df.index)) , index=sim_matrix_df.index) , 'idx')\n",
    "\n",
    "    # Add edges based on the k-nearest neighbors\n",
    "    for node, neighbors in k_neighbors_dict.items():\n",
    "        for neighbor in neighbors:\n",
    "            if node == neighbor:\n",
    "                print('Node {} is its own neighbor'.format(node))\n",
    "\n",
    "            weight = sim_matrix_df.iloc[node, neighbor]\n",
    "            G.add_edge(node, neighbor, weight=weight)\n",
    "            # G.add_edge(node, neighbor)\n",
    "\n",
    "    return G \n",
    "\n",
    "\n",
    "def plot_network(G , label_colours = ['skyblue', 'orange'] , node_size = 50, \n",
    "                 title = 'Patient Similarity Network', with_labels = False) :\n",
    "\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # Define color map based on labels, 0 is non-ASD, 1 is ASD\n",
    "    label_colour_map = {0: label_colours[0] , 1 : label_colours[1]}\n",
    "    node_colours = [label_colour_map[G.nodes[node]['label']] for node in G]\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)  # seed ensures layout is the same each run\n",
    "\n",
    "    # nx.draw(G, pos, with_labels=False, node_color=node_colours,\n",
    "                #  node_size=node_size, font_size=8, alpha=0.3)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colours, node_size=node_size, alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "\n",
    "    if with_labels:\n",
    "        node_labels = nx.get_node_attributes(G, 'idx')  \n",
    "        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8)\n",
    "\n",
    "    # Create a patch list for the legend\n",
    "    label_names = ['No ASD', 'ASD']\n",
    "    legend_patches = [mpatches.Patch(color=colour, label=label) for label, colour in zip(label_names, label_colours)]\n",
    "\n",
    "    plt.legend(handles=legend_patches)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example graph with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 500\n",
    "# k = 20\n",
    "# cosine_sim_df_small = hamming_sim_df_90k.iloc[:n, :n]\n",
    "\n",
    "# # G_knn_small = build_network_knn(cosine_sim_df_small, spark_df_asd.iloc[:n], k_neighbors=k, compute_knn_directly=False)\n",
    "# # title = f\"Patient Similarity Network with KNN (n={n}, k={k})\"\n",
    "# # print_graph_info(G_knn_small)\n",
    "# # plot_network(G_knn_small, node_size=10, title = title)\n",
    "\n",
    "# G_knn_small = build_network_knn(cosine_sim_df_small, spark_df_asd.iloc[:n], k_neighbors=k, compute_knn_directly=True)\n",
    "# title = f\"Patient Similarity Network with KNN (n={n}, k={k})\"\n",
    "# print_graph_info(G_knn_small)\n",
    "# plot_network(G_knn_small, node_size=10, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network_threshold(sim_matrix_df, labels, threshold=0.8, features_df = None):\n",
    "    \"\"\"\n",
    "    Creates a graph from a correlation matrix using a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    sim_matrix_df (pd.DataFrame): DataFrame containing the correlation matrix.\n",
    "    labels (pd.Series): Series containing the labels for each patient.\n",
    "    threshold (float): Threshold for including edges based on similarity value.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Graph created from the similarity matrix.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # # Add nodes\n",
    "    # for node in sim_matrix_df.columns:\n",
    "    #     G.add_node(node)\n",
    "\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    G.add_nodes_from(sim_matrix_df.index)\n",
    "\n",
    "    # nx.set_node_attributes(G , labels , 'label')\n",
    "    nx.set_node_attributes(G , labels , 'asd')\n",
    "    # nx.set_node_attributes(G , pd.Series(np.arange(len(sim_matrix_df.index)) , index=sim_matrix_df.index) , 'idx')\n",
    "\n",
    "    # Add edges with weights above the threshold\n",
    "    for i in range(sim_matrix_df.shape[0]):\n",
    "        for j in range(i + 1, sim_matrix_df.shape[1]):\n",
    "            if i != j:  # Ignore the diagonal elements\n",
    "                weight = sim_matrix_df.iloc[i, j]\n",
    "                if abs(weight) >= threshold:\n",
    "                    G.add_edge(sim_matrix_df.index[i], sim_matrix_df.columns[j], weight=weight)\n",
    "\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10000\n",
    "# n = 100\n",
    "# hamming_sim_df = pairwise_hamming_similarity(spark_df_selected_feat.iloc[:n])\n",
    "# hamming_sim_df = pairwise_weighted_hamming_similarity(spark_df_selected_feat.iloc[:n], feat_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10000\n",
    "# n = 100\n",
    "# NOTE - new best parameter used\n",
    "# t = 0.6\n",
    "\n",
    "# cosine_sim_df_small = hamming_sim_df.iloc[:n, :n]\n",
    "# G_small_threshold = build_network_threshold(cosine_sim_df_small, spark_df_asd.iloc[:n] ,threshold=t, features_df=spark_df_selected_feat)\n",
    "# G_small_threshold = build_network_threshold(cosine_sim_df_small, spark_df_asd.iloc[:n] ,threshold=t)\n",
    "# print_graph_info(G_small_threshold)\n",
    "# calculate_average_clustering(G_small_threshold)\n",
    "\n",
    "# print(G_small_threshold.nodes(data=True))\n",
    "# title = f\"Patient Similarity Network with Threshold (n={n}, threshold={t})\"\n",
    "# plot_network(G_small_threshold, node_size=10, title = title)\n",
    "\n",
    "# 17 mins for 10k patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph(G, degree_threshold=1, keep_largest_component=True):\n",
    "    \"\"\"\n",
    "    Cleans the graph by performing several cleaning steps:\n",
    "    - Removes unconnected nodes (isolates)\n",
    "    - Removes self-loops\n",
    "    - Removes nodes with a degree below a specified threshold\n",
    "    - Keeps only the largest connected component (optional)\n",
    "\n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph to clean.\n",
    "    degree_threshold (int): Minimum degree for nodes to keep.\n",
    "    keep_largest_component (bool): Whether to keep only the largest connected component.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Cleaned graph.\n",
    "    \"\"\"\n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Remove nodes with no edges (isolates)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    # Remove nodes with degree below the threshold\n",
    "    low_degree_nodes = [node for node, degree in dict(G.degree()).items() if degree < degree_threshold]\n",
    "    G.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "    # Keep only the largest connected component\n",
    "    if keep_largest_component:\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G = G.subgraph(largest_cc)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean the graph by removing unconnected nodes\n",
    "# G_small_threshold_cleaned = clean_graph(G_small_threshold,\n",
    "#                                     degree_threshold=5,\n",
    "#                                     keep_largest_component=False)\n",
    "# plot_network(G_small_threshold_cleaned, node_size=10, title=f'Cleaned Patient Similarity Network (n={n}, threshold={t})'.format(n,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louvain Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import SpectralClustering\n",
    "# from sklearn.metrics import adjusted_rand_score, rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# communities = nx.community.louvain_communities(G)\n",
    "\n",
    "def plot_network_clusters(G , communities, node_size = 50, \n",
    "                          title = 'Patient Clusters (Louvain clustering)') :\n",
    "    \n",
    "    '''\n",
    "    G : nx.Graph\n",
    "    communities : list of sets containing the nodes in each community\n",
    "    '''\n",
    "\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Convert communities to an array of cluster labels\n",
    "    cluster_map = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            cluster_map[node] = i  # Assign a unique cluster label based on community\n",
    "\n",
    "    \n",
    "    cluster_labels = [cluster_map[node] for node in G.nodes()]\n",
    "\n",
    "\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)  # seed ensures layout is the same each run\n",
    "\n",
    "\n",
    "    # Create a color map from labels to colors\n",
    "    unique_cluster_labels = np.unique(cluster_labels)\n",
    "    node_colours = [plt.cm.jet(label / max(unique_cluster_labels)) for label in cluster_labels]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colours, node_size=node_size, alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "\n",
    "    # if with_labels:\n",
    "    #     node_labels = nx.get_node_attributes(G, 'idx')  \n",
    "    #     nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8)\n",
    "\n",
    "    # Create a patch list for the legend\n",
    "    legend_handles = [mpatches.Patch(color=plt.cm.jet(i / max(unique_cluster_labels)), label=f'Cluster {i+1}') for i in unique_cluster_labels]\n",
    "\n",
    "    plt.legend(handles=legend_handles)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Process Memory Usage: {mem_info.rss / (1024 ** 3):.2f} GB\")\n",
    "    # print(f\"Total Memory: {virtual_mem.total / (1024 ** 3):.2f} GB\")\n",
    "    # print(f\"Available Memory: {virtual_mem.available / (1024 ** 3):.2f} GB\")\n",
    "    # print(f\"Used Memory: {virtual_mem.used / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"Memory Percentage: {virtual_mem.percent}%\")\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample size: 1000\n",
      "Using k: 100\n",
      "Processing sample 1...\n",
      "Saving results...\n",
      "Total time taken: 0.08 minutes.\n",
      "\n",
      "Processing sample 2...\n",
      "Saving results...\n",
      "Total time taken: 0.08 minutes.\n",
      "\n",
      "Processing sample 3...\n",
      "Saving results...\n",
      "Total time taken: 0.08 minutes.\n",
      "\n",
      "Processing sample 4...\n",
      "Saving results...\n",
      "Total time taken: 0.08 minutes.\n",
      "\n",
      "Processing sample 5...\n",
      "Saving results...\n",
      "Total time taken: 0.08 minutes.\n",
      "\n",
      "Using sample size: 2000\n",
      "Using k: 200\n",
      "Processing sample 1...\n",
      "Saving results...\n",
      "Total time taken: 0.31 minutes.\n",
      "\n",
      "Processing sample 2...\n",
      "Saving results...\n",
      "Total time taken: 0.31 minutes.\n",
      "\n",
      "Processing sample 3...\n",
      "Saving results...\n",
      "Total time taken: 0.30 minutes.\n",
      "\n",
      "Processing sample 4...\n",
      "Saving results...\n",
      "Total time taken: 0.31 minutes.\n",
      "\n",
      "Processing sample 5...\n",
      "Saving results...\n",
      "Total time taken: 0.30 minutes.\n",
      "\n",
      "Using sample size: 4000\n",
      "Using k: 400\n",
      "Processing sample 1...\n",
      "Saving results...\n",
      "Total time taken: 1.24 minutes.\n",
      "\n",
      "Processing sample 2...\n",
      "Saving results...\n",
      "Total time taken: 1.23 minutes.\n",
      "\n",
      "Processing sample 3...\n",
      "Saving results...\n",
      "Total time taken: 1.23 minutes.\n",
      "\n",
      "Processing sample 4...\n",
      "Saving results...\n",
      "Total time taken: 1.24 minutes.\n",
      "\n",
      "Processing sample 5...\n",
      "Saving results...\n",
      "Total time taken: 1.23 minutes.\n",
      "\n",
      "Using sample size: 6000\n",
      "Using k: 600\n",
      "Processing sample 1...\n",
      "Saving results...\n",
      "Total time taken: 2.79 minutes.\n",
      "\n",
      "Processing sample 2...\n",
      "Saving results...\n",
      "Total time taken: 2.78 minutes.\n",
      "\n",
      "Processing sample 3...\n",
      "Saving results...\n",
      "Total time taken: 2.77 minutes.\n",
      "\n",
      "Processing sample 4...\n",
      "Saving results...\n",
      "Total time taken: 2.78 minutes.\n",
      "\n",
      "Processing sample 5...\n",
      "Saving results...\n",
      "Total time taken: 2.81 minutes.\n",
      "\n",
      "Using sample size: 8000\n",
      "Using k: 800\n",
      "Processing sample 1...\n",
      "Saving results...\n",
      "Total time taken: 4.98 minutes.\n",
      "\n",
      "Processing sample 2...\n",
      "Saving results...\n",
      "Total time taken: 4.98 minutes.\n",
      "\n",
      "Processing sample 3...\n",
      "Saving results...\n",
      "Total time taken: 4.90 minutes.\n",
      "\n",
      "Processing sample 4...\n",
      "Saving results...\n",
      "Total time taken: 4.92 minutes.\n",
      "\n",
      "Processing sample 5...\n",
      "Saving results...\n",
      "Total time taken: 4.99 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = [1000, 2000, 4000, 6000, 8000]\n",
    "for sample_size in sample_sizes:\n",
    "    \n",
    "    k = int(sample_size/10)\n",
    "    print(f\"Using sample size: {sample_size}\")\n",
    "    print(f\"Using k: {k}\")\n",
    "    sample_dfs = get_sample_datasets(spark_df_selected_feat, sample_size, 5)\n",
    "\n",
    "    for i, sample_df in enumerate(sample_dfs):\n",
    "\n",
    "        print(f\"Processing sample {i+1}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        x = sample_df.drop(['asd', 'index'], axis=1)\n",
    "        y = sample_df['asd']\n",
    "        sim_df = pairwise_weighted_hamming_similarity(x, feat_importances)\n",
    "        # G = build_network_threshold(sim_df, labels=y ,threshold=t, features_df=x)\n",
    "        G = build_network_knn(sim_df , labels=y,  k_neighbors = k, compute_knn_directly = True)\n",
    "        communities = nx.community.louvain_communities(G)\n",
    "        # plot_network_clusters(G, communities, title='Patient Similarity Network with Clusters')\n",
    "        \n",
    "        cluster_membership = {}\n",
    "        for idx, community in enumerate(communities):\n",
    "            for node_id in community:\n",
    "                cluster_membership[node_id] = idx\n",
    "\n",
    "        sample_df['cluster'] = sample_df.index.map(cluster_membership)\n",
    "\n",
    "        print(\"Saving results...\")\n",
    "        folder_path = f\"sampling_louvain_results_knn/{sample_size}/{i}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Save sample_df\n",
    "        sample_df_path = os.path.join(folder_path, 'sample_df.pkl')\n",
    "        with open(sample_df_path, 'wb') as file:\n",
    "            pickle.dump(sample_df, file)\n",
    "\n",
    "        # Save communities\n",
    "        communities_path = os.path.join(folder_path, 'communities.pkl')\n",
    "        with open(communities_path, 'wb') as file:\n",
    "            pickle.dump(communities, file)\n",
    "\n",
    "        # Save G\n",
    "        g_path = os.path.join(folder_path, 'G.gpickle')\n",
    "        with open(g_path, 'wb') as file:\n",
    "            pickle.dump(G, file)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time_mins = (end_time - start_time)/60\n",
    "        print(f\"Total time taken: {elapsed_time_mins:.2f} minutes.\")\n",
    "        print()\n",
    "\n",
    "        # Clear variables to free up memory\n",
    "        # print_memory_usage()\n",
    "        del G, x, y, sim_df, communities, cluster_membership\n",
    "        gc.collect()\n",
    "        # print(\"garbage collected\")\n",
    "        # print_memory_usage()\n",
    "        # print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample size: 20000\n",
      "Using k: 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sample_size=20000\n",
    "# k = int(sample_size/10)\n",
    "# print(f\"Using sample size: {sample_size}\")\n",
    "# print(f\"Using k: {k}\")\n",
    "# sample_dfs = get_sample_datasets(spark_df_selected_feat, sample_size, 5)\n",
    "\n",
    "# for i, sample_df in enumerate(sample_dfs):\n",
    "\n",
    "#     print(f\"Processing sample {i+1}...\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     x = sample_df.drop(['asd', 'index'], axis=1)\n",
    "#     y = sample_df['asd']\n",
    "#     sim_df = pairwise_weighted_hamming_similarity(x, feat_importances)\n",
    "#     # G = build_network_threshold(sim_df, labels=y ,threshold=t, features_df=x)\n",
    "#     G = build_network_knn(sim_df , labels=y,  k_neighbors = k, compute_knn_directly = True)\n",
    "#     communities = nx.community.louvain_communities(G)\n",
    "#     # plot_network_clusters(G, communities, title='Patient Similarity Network with Clusters')\n",
    "    \n",
    "#     cluster_membership = {}\n",
    "#     for idx, community in enumerate(communities):\n",
    "#         for node_id in community:\n",
    "#             cluster_membership[node_id] = idx\n",
    "\n",
    "#     sample_df['cluster'] = sample_df.index.map(cluster_membership)\n",
    "\n",
    "#     print(\"Saving results...\")\n",
    "#     folder_path = f\"sampling_louvain_results_knn/{sample_size}/{i}\"\n",
    "#     os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "#     # Save sample_df\n",
    "#     sample_df_path = os.path.join(folder_path, 'sample_df.pkl')\n",
    "#     with open(sample_df_path, 'wb') as file:\n",
    "#         pickle.dump(sample_df, file)\n",
    "\n",
    "#     # Save communities\n",
    "#     communities_path = os.path.join(folder_path, 'communities.pkl')\n",
    "#     with open(communities_path, 'wb') as file:\n",
    "#         pickle.dump(communities, file)\n",
    "\n",
    "#     # Save G\n",
    "#     g_path = os.path.join(folder_path, 'G.gpickle')\n",
    "#     with open(g_path, 'wb') as file:\n",
    "#         pickle.dump(G, file)\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time_mins = (end_time - start_time)/60\n",
    "#     print(f\"Total time taken: {elapsed_time_mins:.2f} minutes.\")\n",
    "#     print()\n",
    "\n",
    "#     # Clear variables to free up memory\n",
    "#     # print_memory_usage()\n",
    "#     del G, x, y, sim_df, communities, cluster_membership\n",
    "#     gc.collect()\n",
    "#     # print(\"garbage collected\")\n",
    "#     # print_memory_usage()\n",
    "#     # print()\n",
    "\n",
    "# # 8k - 20 mins/sample; 5 mins/sample for knn\n",
    "# # 6k - 10 mins/sample; 3 mins/sample for knn\n",
    "# # 4k - 5 mins/sample;    1.5 mins/sample for knn\n",
    "# # 2k - 1 mins/sample; 0.5 mins/sample for knn\n",
    "# # 1k - 0.5 mins/sample\n",
    "# # 20k - 130 mins/sample; 33 mins/sample for knn\n",
    "# # 10k - 40ish mins/sample ; 8 mins/sample for knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples processed.\n"
     ]
    }
   ],
   "source": [
    "print(\"All samples processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "# import seaborn as sns\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # import networkx as nx\n",
    "\n",
    "# # Assuming 'G' is your graph and 'communities' is a list of sets, where each set contains the nodes in a community\n",
    "# # Step 1: Extract Node Attributes and Cluster Membership\n",
    "# node_attributes = {node: G.nodes[node] for node in G.nodes()}\n",
    "# # print(node_attributes)\n",
    "# cluster_membership = {node: idx for idx, community in enumerate(communities) for node in community}\n",
    "\n",
    "# # Add cluster membership as an attribute\n",
    "# nx.set_node_attributes(G, cluster_membership, 'cluster')\n",
    "\n",
    "# # Step 2: Aggregate Attributes by Cluster\n",
    "# # Initialize a list to hold aggregated data\n",
    "# aggregated_data = []\n",
    "\n",
    "# # Iterate over each community to aggregate attributes\n",
    "# for idx, community in enumerate(communities):\n",
    "#     # print(idx, community)\n",
    "#     community_attributes = [node_attributes[node] for node in community]\n",
    "#     df = pd.DataFrame(community_attributes)\n",
    "#     df['cluster'] = idx + 1\n",
    "#     aggregated_data.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Concatenate all community data\n",
    "# all_data = pd.concat(aggregated_data)\n",
    "\n",
    "# # all_data = sample_df\n",
    "\n",
    "# # Calculate mean and standard deviation for each attribute by cluster\n",
    "# mean_values = all_data.groupby('cluster').mean()\n",
    "# std_values = all_data.groupby('cluster').std()\n",
    "\n",
    "# print(mean_values)\n",
    "\n",
    "# # Step 3: Prepare Data for Heatmap\n",
    "# # For simplicity, focusing on mean values; you can similarly plot std_values\n",
    "# heatmap_data = mean_values\n",
    "\n",
    "# # Step 4: Plot Heatmap\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.heatmap(heatmap_data, annot=False, cmap='viridis')\n",
    "# plt.title('Mean Values of Node Attributes by Cluster (10k Patients)')\n",
    "# plt.ylabel('Cluster')\n",
    "# plt.xlabel('Attribute')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the number of decimal places\n",
    "# decimal_places = 2\n",
    "\n",
    "# # Round mean_values and std_values\n",
    "# rounded_mean_values = mean_values.round(decimal_places)\n",
    "# rounded_std_values = std_values.round(decimal_places)\n",
    "\n",
    "# # Initialize an empty DataFrame with the same index as mean_values or std_values\n",
    "# formatted_table = pd.DataFrame(index=rounded_mean_values.index)\n",
    "\n",
    "# # Iterate over columns in rounded_mean_values to format \"mean (std)\" strings\n",
    "# for col in rounded_mean_values.columns:\n",
    "#     formatted_table[col] = rounded_mean_values[col].astype(str) + \" (\" + rounded_std_values[col].astype(str) + \")\"\n",
    "\n",
    "# # Display the formatted table\n",
    "# print(formatted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(formatted_table.iloc[:,:14])\n",
    "# print()\n",
    "# print(formatted_table.iloc[:,14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_folder = \"louvain_pickles/mf_10k_weighted/\"\n",
    "\n",
    "# mean_values.to_pickle(pickle_folder + \"mean_values.pkl\")\n",
    "# std_values.to_pickle(pickle_folder + \"std_values.pkl\")\n",
    "# formatted_table.to_pickle(pickle_folder + \"formatted_table.pkl\")\n",
    "# pd.to_pickle(all_data, pickle_folder + 'all_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(pickle_folder + 'communities.pkl', 'wb') as file:\n",
    "#     pickle.dump(communities, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pickle_folder + 'G.pkl', 'wb') as file:\n",
    "#     pickle.dump(G, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
